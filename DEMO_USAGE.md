# AlphaSTomics Demo ä½¿ç”¨æŒ‡å—

æœ¬ç›®å½•åŒ…å«ä¸‰ä¸ªæ¼”ç¤ºè„šæœ¬ï¼Œç”¨äºæµ‹è¯•å’Œå¯¹æ¯”ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶å’Œ MoE é…ç½®ã€‚

## ğŸ“ è„šæœ¬è¯´æ˜

### 1. `demo_gated.py` - Gated Attention å®Œæ•´æµ‹è¯•
æµ‹è¯•å¯ç”¨ Gated Attention çš„å®Œæ•´è®­ç»ƒæµç¨‹ã€‚

**é…ç½®:**
- âœ“ Gated Attention (headwise é—¨æ§)
- âœ“ QK Normalization (RMSNorm)
- âœ— MoE

**æµ‹è¯•å†…å®¹:**
1. å‰å‘ä¼ æ’­æµ‹è¯•
2. å™ªå£°æ¨¡å‹æµ‹è¯•
3. Masked Diffusion æµ‹è¯•
4. è®­ç»ƒæ­¥éª¤æµ‹è¯•
5. å®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•
6. æ‰©æ•£é‡‡æ ·æµ‹è¯•

**è¿è¡Œæ–¹å¼:**
```bash
python demo_gated.py
```

---

### 2. `demo_gated_moe.py` - Gated Attention + MoE å®Œæ•´æµ‹è¯•
æµ‹è¯•åŒæ—¶å¯ç”¨ Gated Attention å’Œ MoE çš„å®Œæ•´è®­ç»ƒæµç¨‹ã€‚

**é…ç½®:**
- âœ“ Gated Attention (headwise é—¨æ§)
- âœ“ QK Normalization (RMSNorm)
- âœ“ MoE (4 ä¸ªä¸“å®¶, top-2 æ¿€æ´»)

**æµ‹è¯•å†…å®¹:**
ä¸ `demo_gated.py` ç›¸åŒï¼Œä½†ä½¿ç”¨ MoE å¢å¼ºçš„ FFNã€‚

**è¿è¡Œæ–¹å¼:**
```bash
python demo_gated_moe.py
```

**ç‰¹ç‚¹:**
- æ€»å‚æ•°é‡å¢åŠ çº¦ 19%
- æ¿€æ´»å‚æ•°é‡å‡å°‘çº¦ 18%ï¼ˆç›¸æ¯” baselineï¼‰
- æ¯ä¸ª token åªæ¿€æ´» 2/4 çš„ä¸“å®¶ç½‘ç»œ

---

### 3. `compare_configs.py` - å››ç§é…ç½®å¯¹æ¯”å®éªŒ
å¯¹æ¯”å››ç§ä¸åŒé…ç½®çš„æ€§èƒ½ã€å‚æ•°é‡å’Œè®­ç»ƒæ—¶é—´ã€‚

**å¯¹æ¯”é…ç½®:**
1. **Baseline**: Linear Attention + æ ‡å‡† FFN
2. **Gated Only**: Gated Attention + æ ‡å‡† FFN
3. **MoE Only**: Linear Attention + MoE
4. **Gated + MoE**: Gated Attention + MoE (æœ€å¼ºé…ç½®)

**è¿è¡Œæ–¹å¼:**
```bash
python compare_configs.py
```

**è¾“å‡ºç¤ºä¾‹:**
```
é…ç½®                                       æ€»å‚æ•°          æ¿€æ´»å‚æ•°         è®­ç»ƒæŸå¤±         éªŒè¯æŸå¤±      
Baseline (Linear Attn + Standard FFN)    707,508      -            473.256      208.295
Gated Attention Only                     444,988      -            507.903      308.592
MoE Only (4 experts, top-2)              1,105,076    841,396      568.310      374.699
Gated + MoE (æœ€å¼ºé…ç½®)                       842,556      578,876      498.796      305.334
```

---

## ğŸ”§ é…ç½®ä¿®æ”¹

æ‰€æœ‰è„šæœ¬ä½¿ç”¨ç±»ä¼¼çš„é…ç½®ç»“æ„ï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹ `DEMO_CONFIG` æˆ– `BASE_CONFIG` æ¥è°ƒæ•´ï¼š

### å¯ç”¨/ç¦ç”¨ Gated Attention
```python
"TransformerLayer_setting": {
    "use_gated_attention": True,  # True/False
    "gate_type": "headwise",      # "headwise" / "elementwise"
    "use_qk_norm": True,          # True/False
}
```

### å¯ç”¨/ç¦ç”¨ MoE
```python
"TransformerLayer_setting": {
    "use_moe": True,                        # True/False
    "num_experts": 4,                       # ä¸“å®¶æ•°é‡
    "moe_top_k": 2,                         # æ¯æ¬¡æ¿€æ´»çš„ä¸“å®¶æ•°
    "moe_load_balance_loss_weight": 0.01,   # è´Ÿè½½å‡è¡¡æŸå¤±æƒé‡
}
```

---

## ğŸ“Š å…³é”®å‘ç°

### å‚æ•°æ•ˆç‡
- **Gated Attention**: ç›¸æ¯” baseline å‚æ•°å‡å°‘ 37%ï¼ˆç§»é™¤ Linear Attention ä¾èµ–ï¼‰
- **MoE**: æ€»å‚æ•°å¢åŠ  56%ï¼Œä½†åªæ¿€æ´» 76% çš„å‚æ•°
- **Gated + MoE**: æ€»å‚æ•°å¢åŠ  19%ï¼Œæ¿€æ´»å‚æ•°å‡å°‘ 18%

### æ€§èƒ½å»ºè®®
1. **ä¼˜å…ˆæ¨è**: Gated Attention Only
   - å‚æ•°å°‘ï¼Œè®­ç»ƒå¿«
   - é€‚åˆä¸­å°è§„æ¨¡æ•°æ®é›†
   
2. **è¿›é˜¶é€‰æ‹©**: Gated + MoE
   - æœ€å¤§æ¨¡å‹å®¹é‡
   - é€‚åˆå¤§è§„æ¨¡ã€å¤æ‚æ•°æ®é›†
   - éœ€è¦æ›´å¤šè°ƒä¼˜

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

1. æµ‹è¯• Gated Attention:
```bash
python demo_gated.py
```

2. æµ‹è¯• Gated Attention + MoE:
```bash
python demo_gated_moe.py
```

3. å¯¹æ¯”æ‰€æœ‰é…ç½®:
```bash
python compare_configs.py
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **æ•°æ®é›†å¤§å°**: MoE éœ€è¦è¶³å¤Ÿå¤§çš„æ•°æ®é›†æ‰èƒ½å……åˆ†åˆ©ç”¨ä¸“å®¶ç½‘ç»œ
2. **è´Ÿè½½å‡è¡¡**: MoE çš„ `load_balance_loss_weight` éœ€è¦æ ¹æ®æ•°æ®é›†è°ƒä¼˜
3. **æ¸è¿›å¼å‡çº§**: å»ºè®®å…ˆæµ‹è¯• Gated Attentionï¼Œç¡®è®¤æœ‰æ•ˆåå†æ·»åŠ  MoE

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `GATED_ATTENTION.md`: Gated Attention è¯¦ç»†è¯´æ˜
- `GATED_MOE_GUIDE.md`: MoE è¯¦ç»†è¯´æ˜å’Œä½¿ç”¨æŒ‡å—
- `config.yaml`: å®Œæ•´çš„æ¨¡å‹é…ç½®ç¤ºä¾‹
