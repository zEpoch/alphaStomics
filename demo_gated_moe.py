#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
AlphaSTomics Gated Attention + MoE Demo
========================================
ä½¿ç”¨ Gated Attention å’Œ MoE æµ‹è¯•æ•´ä¸ªè®­ç»ƒæµç¨‹

æµ‹è¯•å†…å®¹:
1. æ•°æ®ç”Ÿæˆå’Œ DataLoader åˆ›å»º
2. æ¨¡å‹åˆå§‹åŒ–ï¼ˆå¯ç”¨ Gated Attention å’Œ MoEï¼‰
3. è®­ç»ƒå¾ªç¯ (forward + backward)
4. éªŒè¯å¾ªç¯
5. é‡‡æ ·æµ‹è¯•

è¿è¡Œæ–¹å¼:
    python demo_gated_moe.py

ä½œè€…: AlphaSTomics Team
æ—¥æœŸ: 2024
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pathlib import Path
import numpy as np
import logging
from typing import Dict, Optional, Tuple

# å¯¼å…¥ AlphaSTomics æ¨¡å—
from alphastomics.diffusion_model import (
    AlphaSTomicsModule,
    NoiseModel,
    DiffusionSampler,
    MaskGenerator,
    MaskingConfig,
)
from alphastomics.utils.dataholder import DataHolder

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ==================== Demo é…ç½® ====================
DEMO_CONFIG = {
    # è®­ç»ƒæ¨¡å¼: "expr_to_pos", "pos_to_expr", "joint"
    "training_mode": "joint",
    
    # æ‰©æ•£å‚æ•°
    "diffusion": {
        "diffusion_steps": 100,  # Demo ä½¿ç”¨è¾ƒå°‘æ­¥æ•°åŠ å¿«æµ‹è¯•
        "diffusion_noise_schedule": "cosine",
        "nu_expression": 1.0,
        "nu_position": 1.0,
    },
    
    # æŸå¤±å‡½æ•°
    "loss": {
        "lambda_expression": 1.0,
        "lambda_position": 1.0,
        "use_distance_matrix": True,
    },
    
    # Masked Diffusion (å¯å¼€å¯/å…³é—­)
    "masking": {
        "enable": True,  # å¯ç”¨ Masked Diffusion
        "expression_mask_ratio": 0.3,
        "position_mask_ratio": 0.33,
        "mask_strategy": "random",
        "mask_expression": True,
        "mask_position": True,
        "reconstruction_weight": 0.5,
        "masking_probability": 0.5,
        "progressive_masking": False,
    },
    
    # è®­ç»ƒå‚æ•°
    "training": {
        "learning_rate": 1e-3,  # Demo ä½¿ç”¨è¾ƒå¤§å­¦ä¹ ç‡
        "weight_decay": 1e-5,
        "batch_size": 4,
        "epochs": 5,  # Demo åªè®­ç»ƒå‡ ä¸ª epoch
    },
    
    # é‡‡æ ·å‚æ•°
    "sampling": {
        "num_steps": 20,  # Demo ä½¿ç”¨è¾ƒå°‘æ­¥æ•°
    },
    
    # æ¨¡å‹æ¶æ„ (å°å‹æ¨¡å‹ç”¨äºæµ‹è¯•)
    "mlp_in_expression_setting": {
        "mlp_in_expression_dims": 64,
        "mlp_out_expression_dims": 128,
    },
    "mlp_in_diffusion_time_setting": {
        "mlp_in_diffusion_time_dims": 32,
        "mlp_out_diffusion_time_dims": 32,
    },
    "PositionMLP_setting": {
        "hidden_dims": 32,
    },
    "TransformerLayer_setting": {
        "num_layers": 2,  # Demo ä½¿ç”¨è¾ƒå°‘å±‚æ•°
        "num_heads": 4,
        "dim_ff_expression": 256,
        "dim_ff_diffusion_time": 64,
        "dropout": 0.1,
        "layer_norm_eps": 1e-6,
        # Gated Attention å‚æ•°
        "use_gated_attention": True,
        "gate_type": "headwise",  # 'headwise' / 'elementwise' / 'none'
        "use_qk_norm": True,
        # MoE å‚æ•°
        "use_moe": True,
        "num_experts": 4,  # 4 ä¸ªä¸“å®¶
        "moe_top_k": 2,  # æ¯æ¬¡æ¿€æ´» 2 ä¸ªä¸“å®¶
        "moe_load_balance_loss_weight": 0.01,  # è´Ÿè½½å‡è¡¡æƒé‡
    },
    "mlp_out_expression_setting": {
        "hidden_dims": 128,
    },
    "mlp_out_position_norm_setting": {
        "hidden_dims": 64,
    },
}


# ==================== Demo æ•°æ®é›† ====================
class DemoDataset(Dataset):
    """
    Demo æ•°æ®é›†ï¼šç”Ÿæˆæ¨¡æ‹Ÿçš„ç©ºé—´è½¬å½•ç»„æ•°æ®
    
    æ¨¡æ‹Ÿæ•°æ®ç‰¹ç‚¹:
    - è¡¨è¾¾é‡: åŸºå› è¡¨è¾¾çŸ©é˜µ (N_cells, N_genes)
    - ä½ç½®: 3D ç©ºé—´åæ ‡ (N_cells, 3)
    - æ”¯æŒå¯å˜æ•°é‡çš„ç»†èƒ
    """
    
    def __init__(
        self,
        num_samples: int = 100,
        num_genes: int = 50,
        min_cells: int = 20,
        max_cells: int = 100,
        seed: int = 42,
    ):
        """
        åˆå§‹åŒ– Demo æ•°æ®é›†
        
        Args:
            num_samples: æ ·æœ¬æ•°é‡ï¼ˆæ¨¡æ‹Ÿåˆ‡ç‰‡æ•°é‡ï¼‰
            num_genes: åŸºå› æ•°é‡
            min_cells: æ¯ä¸ªæ ·æœ¬æœ€å°‘ç»†èƒæ•°
            max_cells: æ¯ä¸ªæ ·æœ¬æœ€å¤šç»†èƒæ•°
            seed: éšæœºç§å­
        """
        super().__init__()
        self.num_samples = num_samples
        self.num_genes = num_genes
        self.min_cells = min_cells
        self.max_cells = max_cells
        
        np.random.seed(seed)
        torch.manual_seed(seed)
        
        # é¢„ç”Ÿæˆæ‰€æœ‰æ•°æ®
        self.data = []
        for i in range(num_samples):
            sample = self._generate_sample()
            self.data.append(sample)
    
    def _generate_sample(self) -> Dict:
        """
        ç”Ÿæˆå•ä¸ªæ¨¡æ‹Ÿæ ·æœ¬
        
        æ¨¡æ‹Ÿç­–ç•¥:
        - ä½ç½®: åœ¨ 3D ç©ºé—´ä¸­çš„èšç±»åˆ†å¸ƒï¼ˆæ¨¡æ‹Ÿç»„ç»‡ç»“æ„ï¼‰
        - è¡¨è¾¾é‡: æ ¹æ®ä½ç½®ç”Ÿæˆï¼ˆæ¨¡æ‹Ÿç©ºé—´åŸºå› è¡¨è¾¾æ¨¡å¼ï¼‰
        """
        # éšæœºç»†èƒæ•°é‡
        num_cells = np.random.randint(self.min_cells, self.max_cells + 1)
        
        # ç”Ÿæˆ 3D ä½ç½® (æ¨¡æ‹Ÿç»„ç»‡åˆ‡ç‰‡)
        # ä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ç”Ÿæˆèšç±»
        num_clusters = np.random.randint(2, 5)
        cluster_centers = np.random.randn(num_clusters, 3) * 2
        
        positions = []
        cluster_labels = []
        cells_per_cluster = num_cells // num_clusters
        
        for c in range(num_clusters):
            if c < num_clusters - 1:
                n_c = cells_per_cluster
            else:
                # æœ€åä¸€ä¸ª cluster è¡¥è¶³å‰©ä½™çš„ç»†èƒæ•°
                n_c = num_cells - sum(p.shape[0] for p in positions)
            n_c = max(1, n_c)  # ç¡®ä¿è‡³å°‘æœ‰ 1 ä¸ªç»†èƒ
            pos_c = cluster_centers[c] + np.random.randn(n_c, 3) * 0.5
            positions.append(pos_c)
            cluster_labels.extend([c] * n_c)
        
        positions = np.vstack(positions)
        cluster_labels = np.array(cluster_labels)
        
        # ç¡®ä¿æ•°ç»„å¤§å°åŒ¹é…
        num_cells = positions.shape[0]
        
        # ä¸­å¿ƒåŒ–ä½ç½®
        positions = positions - positions.mean(axis=0)
        
        # ç”Ÿæˆè¡¨è¾¾é‡ (ä¸ä½ç½®/èšç±»ç›¸å…³)
        expression = np.zeros((num_cells, self.num_genes))
        
        # æ¯ä¸ªèšç±»æœ‰ä¸åŒçš„è¡¨è¾¾æ¨¡å¼
        for c in range(num_clusters):
            mask = cluster_labels == c
            # åŸºç¡€è¡¨è¾¾
            base_expr = np.random.rand(self.num_genes) * 2
            # æ·»åŠ å™ªå£°
            expression[mask] = base_expr + np.random.randn(mask.sum(), self.num_genes) * 0.3
        
        # æ·»åŠ ç©ºé—´æ¢¯åº¦æ•ˆåº”
        for g in range(min(10, self.num_genes)):  # å‰10ä¸ªåŸºå› æœ‰ç©ºé—´æ¢¯åº¦
            gradient = positions[:, g % 3]  # ä½¿ç”¨æŸä¸ªåæ ‡è½´
            expression[:, g] += gradient * 0.5
        
        # ç¡®ä¿éè´Ÿ
        expression = np.maximum(expression, 0)
        
        return {
            "expression": torch.tensor(expression, dtype=torch.float32),
            "positions": torch.tensor(positions, dtype=torch.float32),
            "num_cells": num_cells,
            "cluster_labels": torch.tensor(cluster_labels, dtype=torch.long),
        }
    
    def __len__(self) -> int:
        return self.num_samples
    
    def __getitem__(self, idx: int) -> Dict:
        return self.data[idx]


def collate_fn(batch: list) -> Dict:
    """
    å°†ä¸åŒå¤§å°çš„æ ·æœ¬ padding æˆç»Ÿä¸€çš„ batch
    
    Returns:
        dict with:
            - expression: (B, max_N, G)
            - positions: (B, max_N, 3)
            - node_mask: (B, max_N) - 1 è¡¨ç¤ºæœ‰æ•ˆç»†èƒï¼Œ0 è¡¨ç¤º padding
    """
    batch_size = len(batch)
    num_genes = batch[0]["expression"].shape[1]
    
    # æ‰¾åˆ°æœ€å¤§ç»†èƒæ•°
    max_cells = max(sample["num_cells"] for sample in batch)
    
    # åˆå§‹åŒ– padded tensors
    expression = torch.zeros(batch_size, max_cells, num_genes)
    positions = torch.zeros(batch_size, max_cells, 3)
    node_mask = torch.zeros(batch_size, max_cells)
    
    # å¡«å……æ•°æ®
    for i, sample in enumerate(batch):
        n = sample["num_cells"]
        expression[i, :n, :] = sample["expression"]
        positions[i, :n, :] = sample["positions"]
        node_mask[i, :n] = 1.0
    
    return {
        "expression": expression,
        "positions": positions,
        "node_mask": node_mask,
    }


# ==================== Demo DataModule ====================
class DemoDataModule(pl.LightningDataModule):
    """PyTorch Lightning DataModule for demo data"""
    
    def __init__(
        self,
        num_genes: int = 50,
        batch_size: int = 4,
        num_train: int = 80,
        num_val: int = 10,
        num_test: int = 10,
    ):
        super().__init__()
        self.num_genes = num_genes
        self.batch_size = batch_size
        self.num_train = num_train
        self.num_val = num_val
        self.num_test = num_test
    
    def setup(self, stage: Optional[str] = None):
        """åˆ›å»ºæ•°æ®é›†"""
        self.train_dataset = DemoDataset(
            num_samples=self.num_train,
            num_genes=self.num_genes,
            seed=42,
        )
        self.val_dataset = DemoDataset(
            num_samples=self.num_val,
            num_genes=self.num_genes,
            seed=123,
        )
        self.test_dataset = DemoDataset(
            num_samples=self.num_test,
            num_genes=self.num_genes,
            seed=456,
        )
    
    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            collate_fn=collate_fn,
            num_workers=4,
            persistent_workers=True,
        )
    
    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            collate_fn=collate_fn,
            num_workers=4,
            persistent_workers=True,
        )
    
    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            collate_fn=collate_fn,
            num_workers=4,
            persistent_workers=True,
        )


# ==================== ä¸»å‡½æ•° ====================
def test_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯• 1: å‰å‘ä¼ æ’­")
    logger.info("=" * 60)
    
    num_genes = 50
    batch_size = 2
    num_cells = 30
    
    # åˆ›å»ºæ¨¡å‹
    model = AlphaSTomicsModule(cfg=DEMO_CONFIG, num_genes=num_genes)
    print(model)
    # åˆ›å»ºå‡æ•°æ®
    expression = torch.randn(batch_size, num_cells, num_genes)
    positions = torch.randn(batch_size, num_cells, 3)
    diffusion_time = torch.rand(batch_size, 1)
    node_mask = torch.ones(batch_size, num_cells)
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        output = model(
            expression=expression,
            positions=positions,
            diffusion_time=diffusion_time,
            node_mask=node_mask,
        )
        # å¤„ç†è¿”å›å€¼ï¼šå¯èƒ½æ˜¯ 2 ä¸ªæˆ– 3 ä¸ªå€¼
        if len(output) == 3:
            pred_expr, pred_pos, moe_aux_loss = output
            logger.info(f"   MoE è¾…åŠ©æŸå¤±: {moe_aux_loss.item() if moe_aux_loss is not None else 'N/A'}")
        else:
            pred_expr, pred_pos = output
    
    logger.info(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")
    logger.info(f"   è¾“å…¥ expression: {expression.shape}")
    logger.info(f"   è¾“å…¥ positions: {positions.shape}")
    logger.info(f"   è¾“å‡º pred_expression: {pred_expr.shape}")
    logger.info(f"   è¾“å‡º pred_positions: {pred_pos.shape}")
    
    return True


def test_noise_model():
    """æµ‹è¯•å™ªå£°æ¨¡å‹"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯• 2: å™ªå£°æ¨¡å‹")
    logger.info("=" * 60)
    
    num_genes = 50
    batch_size = 2
    num_cells = 30
    
    # åˆ›å»ºå™ªå£°æ¨¡å‹
    noise_model = NoiseModel(DEMO_CONFIG["diffusion"])
    
    # åˆ›å»ºåŸå§‹æ•°æ®
    data = DataHolder(
        expression=torch.randn(batch_size, num_cells, num_genes),
        positions=torch.randn(batch_size, num_cells, 3),
        node_mask=torch.ones(batch_size, num_cells),
    )
    
    # åº”ç”¨å™ªå£°
    noisy_data, mask_info = noise_model.apply_noise(
        data,
        noise_expression=True,
        noise_position=True,
        masking_module=None,
        apply_masking=False,
    )
    
    logger.info(f"âœ… å™ªå£°æ¨¡å‹æµ‹è¯•æˆåŠŸ!")
    logger.info(f"   åŸå§‹ expression: {data.expression.shape}")
    logger.info(f"   åŠ å™ª expression: {noisy_data.noisy_expression.shape}")
    logger.info(f"   æ‰©æ•£æ—¶é—´: {noisy_data.diffusion_time.shape}")
    
    return True


def test_masked_diffusion():
    """æµ‹è¯• Masked Diffusion"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯• 3: Masked Diffusion")
    logger.info("=" * 60)
    
    num_genes = 50
    batch_size = 2
    num_cells = 30
    
    # åˆ›å»º MaskGeneratorï¼ˆæ³¨æ„ï¼šæ¥å£åªéœ€è¦ mask é…ç½®å‚æ•°ï¼‰
    mask_generator = MaskGenerator(
        expression_mask_ratio=0.3,
        position_mask_ratio=0.33,
        mask_strategy='random',
    )
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    expression = torch.randn(batch_size, num_cells, num_genes)
    positions = torch.randn(batch_size, num_cells, 3)
    node_mask = torch.ones(batch_size, num_cells)
    
    # ç”Ÿæˆ expression mask
    expr_mask = mask_generator.generate_expression_mask(expression)
    pos_mask = mask_generator.generate_position_mask(positions)
    
    logger.info(f"âœ… Masked Diffusion æµ‹è¯•æˆåŠŸ!")
    logger.info(f"   Expression mask shape: {expr_mask.shape}")
    logger.info(f"   Position mask shape: {pos_mask.shape}")
    logger.info(f"   Expression mask ratio: {expr_mask.float().mean().item():.2%}")
    logger.info(f"   Position mask ratio: {pos_mask.float().mean().item():.2%}")
    
    # åº”ç”¨ maskï¼ˆå°† masked ä½ç½®è®¾ä¸º 0ï¼‰
    masked_expr = expression.clone()
    masked_expr[expr_mask] = 0
    masked_pos = positions.clone()
    masked_pos[pos_mask] = 0
    
    logger.info(f"   Masked expression: {masked_expr.shape}")
    logger.info(f"   Masked positions: {masked_pos.shape}")
    
    return True


def test_training_step():
    """æµ‹è¯•å•æ­¥è®­ç»ƒ"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯• 4: è®­ç»ƒæ­¥éª¤ (forward + backward)")
    logger.info("=" * 60)
    
    num_genes = 50
    batch_size = 2
    num_cells = 30
    
    # åˆ›å»ºæ¨¡å‹
    model = AlphaSTomicsModule(cfg=DEMO_CONFIG, num_genes=num_genes)
    model.train()
    
    # åˆ›å»ºå‡ batchï¼Œè½¬æ¢ä¸º DataHolder é¿å… self.log() è­¦å‘Š
    from alphastomics.utils.dataholder import DataHolder
    batch = DataHolder(
        expression=torch.randn(batch_size, num_cells, num_genes),
        positions=torch.randn(batch_size, num_cells, 3),
        node_mask=torch.ones(batch_size, num_cells),
    )
    
    # æ‰‹åŠ¨æ‰§è¡Œè®­ç»ƒæ­¥éª¤ï¼ˆä¸ä½¿ç”¨ training_step ä»¥é¿å… log è­¦å‘Šï¼‰
    import warnings
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        loss = model.training_step(batch, batch_idx=0)
    
    logger.info(f"âœ… è®­ç»ƒæ­¥éª¤æˆåŠŸ!")
    logger.info(f"   Loss: {loss.item():.4f}")
    
    # åå‘ä¼ æ’­æµ‹è¯•
    loss.backward()
    logger.info(f"âœ… åå‘ä¼ æ’­æˆåŠŸ!")
    
    return True


def test_full_training():
    """æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯• 5: å®Œæ•´è®­ç»ƒæµç¨‹ (PyTorch Lightning)")
    logger.info("=" * 60)
    
    num_genes = 50
    batch_size = 4
    
    # åˆ›å»º DataModule
    datamodule = DemoDataModule(
        num_genes=num_genes,
        batch_size=batch_size,
        num_train=20,  # å°æ•°æ®é›†å¿«é€Ÿæµ‹è¯•
        num_val=5,
        num_test=5,
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = AlphaSTomicsModule(cfg=DEMO_CONFIG, num_genes=num_genes)
    
    # åˆ›å»º Trainer
    trainer = pl.Trainer(
        max_epochs=2,  # åªè®­ç»ƒ 2 ä¸ª epoch
        accelerator="auto",
        devices=1,
        enable_progress_bar=True,
        enable_model_summary=True,
        logger=False,  # ç¦ç”¨æ—¥å¿—è®°å½•
        enable_checkpointing=False,  # ç¦ç”¨æ£€æŸ¥ç‚¹
        num_sanity_val_steps=1,
    )
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("å¼€å§‹è®­ç»ƒ...")
    trainer.fit(model, datamodule=datamodule)
    
    logger.info(f"âœ… å®Œæ•´è®­ç»ƒæµç¨‹æˆåŠŸ!")
    
    return True


def test_sampling():
    """æµ‹è¯•é‡‡æ ·"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯• 6: æ‰©æ•£é‡‡æ ·")
    logger.info("=" * 60)
    
    num_genes = 50
    batch_size = 2
    num_cells = 30
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆç¦ç”¨ masking ä»¥ç®€åŒ–æµ‹è¯•ï¼‰
    sampling_config = DEMO_CONFIG.copy()
    sampling_config["masking"] = {"enable": False}
    
    model = AlphaSTomicsModule(cfg=sampling_config, num_genes=num_genes)
    model.eval()
    
    # åˆ›å»ºé‡‡æ ·å™¨
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    # é¦–å…ˆæµ‹è¯•ç›´æ¥å‰å‘ä¼ æ’­
    logger.info("æµ‹è¯•å‰å‘ä¼ æ’­ (ä½¿ç”¨ (B, 1) æ—¶é—´)...")
    expression = torch.randn(batch_size, num_cells, num_genes).to(device)
    positions = torch.randn(batch_size, num_cells, 3).to(device)
    node_mask = torch.ones(batch_size, num_cells).to(device)
    diffusion_time = torch.ones(batch_size, 1).to(device)  # (B, 1)
    
    with torch.no_grad():
        output = model.model(
            expression_features=expression,
            diffusion_time=diffusion_time,
            position_features=positions,
            node_mask=node_mask,
        )
        # å¤„ç†è¿”å›å€¼ï¼šå¯èƒ½æ˜¯ 2 ä¸ªæˆ– 3 ä¸ªå€¼
        if len(output) == 3:
            pred_expr, pred_pos, _ = output
        else:
            pred_expr, pred_pos = output
    logger.info(f"  å‰å‘ä¼ æ’­æˆåŠŸ! pred_expr: {pred_expr.shape}, pred_pos: {pred_pos.shape}")
    
    # åˆ›å»ºé‡‡æ ·å™¨
    sampler = DiffusionSampler(
        model=model.model,
        noise_model=model.noise_model,
        device=device,
    )
    
    # æ‰§è¡Œé‡‡æ · (expr_to_pos: ä»è¡¨è¾¾é‡é¢„æµ‹ä½ç½®)
    logger.info("æ‰§è¡Œé‡‡æ · (expr_to_pos)...")
    with torch.no_grad():
        sampled_expr, sampled_pos = sampler.sample(
            expression=expression,
            positions=positions,
            node_mask=node_mask,
            mode="expr_to_pos",
            num_steps=10,  # å°‘é‡æ­¥æ•°å¿«é€Ÿæµ‹è¯•
            verbose=True,
        )
    
    logger.info(f"âœ… é‡‡æ ·æˆåŠŸ!")
    logger.info(f"   é‡‡æ ·å¾—åˆ°çš„ expression: {sampled_expr.shape}")
    logger.info(f"   é‡‡æ ·å¾—åˆ°çš„ positions: {sampled_pos.shape}")
    
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("\n" + "=" * 60)
    logger.info("AlphaSTomics Demo è®­ç»ƒæµ‹è¯•")
    logger.info("=" * 60 + "\n")
    
    tests = [
        ("å‰å‘ä¼ æ’­", test_forward_pass),
        ("å™ªå£°æ¨¡å‹", test_noise_model),
        ("Masked Diffusion", test_masked_diffusion),
        ("è®­ç»ƒæ­¥éª¤", test_training_step),
        ("å®Œæ•´è®­ç»ƒ", test_full_training),
        ("æ‰©æ•£é‡‡æ ·", test_sampling),
    ]
    
    results = {}
    for name, test_fn in tests:
        try:
            success = test_fn()
            results[name] = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        except Exception as e:
            results[name] = f"âŒ é”™è¯¯: {str(e)}"
            logger.error(f"æµ‹è¯• '{name}' å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # æ‰“å°æ€»ç»“
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯•æ€»ç»“")
    logger.info("=" * 60)
    for name, result in results.items():
        logger.info(f"  {name}: {result}")
    
    all_passed = all("é€šè¿‡" in r for r in results.values())
    if all_passed:
        logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! è®­ç»ƒæµç¨‹å¯ä»¥æ­£å¸¸è¿è¡Œã€‚")
    else:
        logger.info("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
    
    return all_passed


if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥æ˜¯å¦åªè¿è¡Œç‰¹å®šæµ‹è¯•
    if len(sys.argv) > 1:
        test_name = sys.argv[1]
        if test_name == "forward":
            test_forward_pass()
        elif test_name == "noise":
            test_noise_model()
        elif test_name == "mask":
            test_masked_diffusion()
        elif test_name == "step":
            test_training_step()
        elif test_name == "train":
            test_full_training()
        elif test_name == "sample":
            test_sampling()
        else:
            print(f"æœªçŸ¥æµ‹è¯•: {test_name}")
            print("å¯ç”¨æµ‹è¯•: forward, noise, mask, step, train, sample")
    else:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        run_all_tests()
