# AlphaSTomics - åŒæ¨¡æ€ç©ºé—´è½¬å½•ç»„æ‰©æ•£æ¨¡å‹

<p align="center">
  <b>åŒæ—¶å¯¹åŸºå› è¡¨è¾¾é‡å’Œ3Dç©ºé—´åæ ‡è¿›è¡Œæ‰©æ•£å»ºæ¨¡</b>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.12+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red.svg" alt="PyTorch">
  <img src="https://img.shields.io/badge/Lightning-2.0+-purple.svg" alt="Lightning">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
</p>

---

## ğŸ¯ é¡¹ç›®ç®€ä»‹

AlphaSTomics æ˜¯ä¸€ä¸ªå…ˆè¿›çš„ç©ºé—´è½¬å½•ç»„å­¦æ•°æ®å»ºæ¨¡æ¡†æ¶ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹å’Œ Transformer æ¶æ„ã€‚æ”¯æŒ**åŒæ¨¡æ€æ‰©æ•£**ï¼Œå¯ä»¥åŒæ—¶å¯¹åŸºå› è¡¨è¾¾é‡å’Œç©ºé—´åæ ‡è¿›è¡Œç”Ÿæˆã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ”„ **åŒæ¨¡æ€ç”Ÿæˆ**: åŒæ—¶æ”¯æŒè¡¨è¾¾é‡å’Œä½ç½®çš„ç”Ÿæˆ
- ğŸ›ï¸ **çµæ´»é‡‡æ ·æ¨¡å¼**: è¡¨è¾¾é‡â†’ä½ç½®ã€ä½ç½®â†’è¡¨è¾¾é‡ã€è”åˆç”Ÿæˆ
- ğŸ”’ **å‡ ä½•ä¸å˜æ€§**: ä½ç½®æŸå¤±ä½¿ç”¨è·ç¦»çŸ©é˜µï¼Œä¿è¯æ—‹è½¬å¹³ç§»ä¸å˜æ€§
- âš¡ **é«˜æ•ˆæ³¨æ„åŠ›**: ä½¿ç”¨ Linear Attentionï¼Œå¤æ‚åº¦ O(n) æ”¯æŒå¤§è§„æ¨¡æ•°æ®
- ğŸ“ **3D CCF åæ ‡**: æ”¯æŒé…å‡†åçš„ä¸‰ç»´ç©ºé—´åæ ‡ï¼ˆåŸ LUNA ä»…æ”¯æŒ 2Dï¼‰
- ğŸŒ **å¤šå¹³å°æ”¯æŒ**: å…¼å®¹ StereoSeqã€MERFISHã€MERSCOPE ç­‰å¤šç§å¹³å°æ•°æ®
- ğŸš€ **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒå¤šæœºå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒï¼ˆDDPã€FSDPã€DeepSpeedï¼‰

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
alphastomics/
â”œâ”€â”€ __init__.py                    # åŒ…å…¥å£
â”œâ”€â”€ main.py                        # ä¸»ç¨‹åºå…¥å£ï¼ˆCLIï¼‰
â”œâ”€â”€ config.yaml                    # é…ç½®æ–‡ä»¶
â”œâ”€â”€ attn_model/                    # æ³¨æ„åŠ›æ¨¡å‹
â”‚   â”œâ”€â”€ layers.py                  # åŸºç¡€å±‚ï¼ˆMLP, PositionMLP, PositionNormï¼‰
â”‚   â”œâ”€â”€ model.py                   # ä¸»æ¨¡å‹
â”‚   â”œâ”€â”€ self_attention.py          # è‡ªæ³¨æ„åŠ›ï¼ˆLinear Attentionï¼‰
â”‚   â””â”€â”€ transformer.py             # Transformer å±‚
â”œâ”€â”€ diffusion_model/               # æ‰©æ•£æ¨¡å‹
â”‚   â”œâ”€â”€ diffusion_utils.py         # æ‰©æ•£å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ noise_model.py             # åŒæ¨¡æ€å™ªå£°æ¨¡å‹
â”‚   â”œâ”€â”€ loss.py                    # åŒæ¨¡æ€æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ sample.py                  # é‡‡æ ·å™¨
â”‚   â””â”€â”€ train.py                   # PyTorch Lightning è®­ç»ƒæ¨¡å—
â”œâ”€â”€ utils/                         # å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ dataholder.py              # ç»Ÿä¸€æ•°æ®å®¹å™¨
â”‚   â”œâ”€â”€ dataloader.py              # æ•°æ®åŠ è½½å™¨ï¼ˆåˆ‡ç‰‡çº§/ç»†èƒçº§ï¼‰
â”‚   â”œâ”€â”€ metrics.py                 # è¯„ä¼°æŒ‡æ ‡
â”‚   â””â”€â”€ embedding_extractor.py     # Embedding æå–å·¥å…·
â””â”€â”€ scripts/
    â””â”€â”€ run_distributed.sh         # åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬
```

---

## ğŸ”§ ç›¸å¯¹äº LUNA çš„ä¼˜åŒ–ä¸æ”¹è¿›

### 1. æ¶æ„å±‚é¢çš„æ”¹è¿›

| æ¨¡å— | LUNA | AlphaSTomics | æ”¹è¿›è¯´æ˜ |
|------|------|--------------|---------|
| **åæ ‡ç»´åº¦** | 2D | 3D | æ”¯æŒä¸‰ç»´ç©ºé—´è½¬å½•ç»„æ•°æ® |
| **æ‰©æ•£æ¨¡æ€** | ä»…ä½ç½® | è¡¨è¾¾é‡ + ä½ç½® | åŒæ¨¡æ€è”åˆæ‰©æ•£ |
| **æ³¨æ„åŠ›ç±»å‹** | æ ‡å‡† Attention | Linear Attention | O(nÂ²) â†’ O(n)ï¼Œæ”¯æŒæ›´å¤šç»†èƒ |
| **é‡‡æ ·æ¨¡å¼** | å•å‘ï¼ˆè¡¨è¾¾é‡â†’ä½ç½®ï¼‰ | åŒå‘ + è”åˆ | æ›´çµæ´»çš„ç”Ÿæˆèƒ½åŠ› |

### 2. ä»£ç ä¿®å¤

#### âœ… ä¿®å¤ï¼šä½ç½®èŒƒæ•°è¾“å‡ºç»´åº¦é”™è¯¯

**é—®é¢˜**: åŸä»£ç ä¸­ `mlp_out_position_norm` è¾“å‡º 3 ç»´å‘é‡ï¼Œä½†åº”è¯¥è¾“å‡º 1 ç»´æ ‡é‡èŒƒæ•°

```python
# âŒ åŸä»£ç ï¼ˆé”™è¯¯ï¼‰
self.mlp_out_position_norm = nn.Sequential(
    nn.Linear(4, hidden_dims),
    nn.Linear(hidden_dims, 3)  # è¾“å‡º 3 ç»´ - é”™è¯¯ï¼
)
new_pos = pos * (new_norm / norm)  # new_norm æ˜¯å‘é‡ï¼Œè¯­ä¹‰é”™è¯¯

# âœ… ä¿®å¤åï¼ˆæ­£ç¡®ï¼Œä¸ LUNA ä¸€è‡´ï¼‰
self.mlp_out_position_norm = nn.Sequential(
    nn.Linear(hidden_dims + 3 + 1, hidden_dims),  # ç‰¹å¾ + ä½ç½® + èŒƒæ•°
    nn.Linear(hidden_dims, 1)  # è¾“å‡º 1 ç»´èŒƒæ•°æ ‡é‡
)
new_pos = pos * new_norm / (norm + eps)  # æ­£ç¡®çš„èŒƒæ•°ç¼©æ”¾
```

#### âœ… ä¿®å¤ï¼šä½ç½®ç¼–ç ä¸¢å¤±è·ç¦»ä¿¡æ¯

**é—®é¢˜**: åŸä»£ç åªä¿ç•™æ–¹å‘ï¼Œä¸¢å¤±äº†è·ç¦»ä¿¡æ¯

```python
# âŒ åŸä»£ç ï¼ˆä¸¢å¤±è·ç¦»ï¼‰
normalized_positions = positions / (norm + 1e-7)  # åªæœ‰æ–¹å‘
transformed = mlp(normalized_positions)  # 3 ç»´è¾“å…¥

# âœ… ä¿®å¤åï¼ˆä¿ç•™æ–¹å‘ + è·ç¦»ï¼‰
direction = positions / (norm + 1e-7)  # æ–¹å‘
features = torch.cat([direction, norm], dim=-1)  # æ–¹å‘ + è·ç¦» = 4 ç»´
transformed = mlp(features)
```

#### âœ… ä¿®å¤ï¼šLayerNorm eps è¢«æ³¨é‡Š

```python
# âŒ åŸä»£ç 
self.norm = LayerNorm(dim)  # eps è¢«æ³¨é‡Šæ‰

# âœ… ä¿®å¤å
self.norm = LayerNorm(dim, eps=layer_norm_eps)  # æ¢å¤ eps å‚æ•°
```

#### âœ… ä¿®å¤ï¼šç¼ºå°‘ mask å¤„ç†å’Œä½ç½®ä¸­å¿ƒåŒ–

```python
# âœ… æ·»åŠ  mask å¤„ç†
out_expression = out_expression * node_mask.unsqueeze(-1)
out_position = out_position * node_mask.unsqueeze(-1)

# âœ… æ·»åŠ ä½ç½®ä¸­å¿ƒåŒ–
mean_pos = (out_position * mask).sum(dim=1) / mask.sum(dim=1)
out_position = (out_position - mean_pos) * mask
```

### 3. æ–°å¢åŠŸèƒ½

#### ğŸ†• åŒæ¨¡æ€å™ªå£°æ¨¡å‹ (`noise_model.py`)

æ”¯æŒåŒæ—¶å¯¹è¡¨è¾¾é‡å’Œä½ç½®åŠ å™ªï¼š

```python
# å‰å‘æ‰©æ•£
z_t^{expr} = Î±_t Â· x_{expr} + Ïƒ_t Â· Îµ_{expr}
z_t^{pos}  = Î±_t Â· x_{pos}  + Ïƒ_t Â· Îµ_{pos}

# å¯ä»¥ç‹¬ç«‹æ§åˆ¶
noisy_data = noise_model.apply_noise(
    data,
    noise_expression=True,   # æ˜¯å¦å¯¹è¡¨è¾¾é‡åŠ å™ª
    noise_position=True      # æ˜¯å¦å¯¹ä½ç½®åŠ å™ª
)
```

#### ğŸ†• åŒæ¨¡æ€æŸå¤±å‡½æ•° (`loss.py`)

```python
# æ€»æŸå¤±
L = Î»_expr Â· L_expression + Î»_pos Â· L_position

# è¡¨è¾¾é‡æŸå¤±: MSE
L_expression = MSE(pred_expr, true_expr)

# ä½ç½®æŸå¤±: è·ç¦»çŸ©é˜µ MSEï¼ˆæ—‹è½¬å¹³ç§»ä¸å˜ï¼‰
D_pred[i,j] = ||pos_pred[i] - pos_pred[j]||
D_true[i,j] = ||pos_true[i] - pos_true[j]||
L_position = MSE(D_pred, D_true)
```

#### ğŸ†• çµæ´»çš„é‡‡æ ·å™¨ (`sample.py`)

æ”¯æŒä¸‰ç§é‡‡æ ·æ¨¡å¼ï¼š

| æ¨¡å¼ | è¾“å…¥æ¡ä»¶ | ç”Ÿæˆç›®æ ‡ | åº”ç”¨åœºæ™¯ |
|------|---------|---------|---------|
| `expr_to_pos` | è¡¨è¾¾é‡ï¼ˆæ¸…æ™°ï¼‰ | ä½ç½® | åŸå§‹ LUNA ä»»åŠ¡ï¼šä»åŸºå› è¡¨è¾¾é‡å»ºç»„ç»‡ç»“æ„ |
| `pos_to_expr` | ä½ç½®ï¼ˆæ¸…æ™°ï¼‰ | è¡¨è¾¾é‡ | ç©ºé—´è½¬å½•ç»„æ¨æ–­ï¼šä»ä½ç½®é¢„æµ‹åŸºå› è¡¨è¾¾ |
| `joint` | ä¸¤è€…éƒ½æ˜¯å™ªå£° | ä¸¤è€… | å®Œå…¨ç”Ÿæˆï¼šç”Ÿæˆè™šæ‹Ÿç»„ç»‡ |

```python
sampler = DiffusionSampler(model, noise_model)

# è¡¨è¾¾é‡ â†’ ä½ç½®ï¼ˆåŸå§‹ä»»åŠ¡ï¼‰
_, positions = sampler.sample(expr, pos, mask, mode="expr_to_pos")

# ä½ç½® â†’ è¡¨è¾¾é‡ï¼ˆæ–°ä»»åŠ¡ï¼‰
expression, _ = sampler.sample(expr, pos, mask, mode="pos_to_expr")

# è”åˆç”Ÿæˆï¼ˆå®Œå…¨ç”Ÿæˆï¼‰
expression, positions = sampler.sample(expr, pos, mask, mode="joint")
```

#### ğŸ†• ç»Ÿä¸€æ•°æ®å®¹å™¨ (`dataholder.py`)

```python
data = DataHolder(
    expression=expr,          # (B, N, G) è¡¨è¾¾é‡
    positions=pos,            # (B, N, 3) 3D ä½ç½®
    node_mask=mask,           # (B, N) æœ‰æ•ˆèŠ‚ç‚¹æ©ç 
    noisy_expression=...,     # åŠ å™ªåçš„è¡¨è¾¾é‡
    noisy_positions=...,      # åŠ å™ªåçš„ä½ç½®
    diffusion_time=t,         # æ‰©æ•£æ—¶é—´
)
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install torch pytorch-lightning
pip install linear_attention_transformer
pip install pyyaml wandb scanpy anndata
```

### 1. æ•°æ®é¢„å¤„ç†

```bash
# ä» h5ad æ–‡ä»¶é¢„å¤„ç†æ•°æ®
python -m alphastomics.main preprocess \
    --input_dir /path/to/h5ad_files \
    --output_dir /path/to/processed \
    --gene_list_file /path/to/gene_list.txt \
    --position_key ccf
```

**å‚æ•°è¯´æ˜ï¼š**
- `--gene_list_file`: å›ºå®šåŸºå› åˆ—è¡¨æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªåŸºå› åï¼‰ï¼Œæ”¯æŒå¤šå¹³å°æ•°æ®ç»Ÿä¸€
- `--position_key`: åæ ‡åœ¨ `adata.obsm` ä¸­çš„ keyï¼ˆé»˜è®¤ `ccf`ï¼Œå·²é…å‡†çš„ 3D åæ ‡ï¼‰

### 2. è®­ç»ƒæ¨¡å‹

**å•å¡è®­ç»ƒï¼š**
```bash
python -m alphastomics.main train \
    --config config.yaml \
    --data_dir /path/to/processed \
    --data_mode slice  # æˆ– cell
```

**å•æœºå¤šå¡è®­ç»ƒï¼š**
```bash
torchrun --nproc_per_node=4 \
    -m alphastomics.main train \
    --config config.yaml \
    --data_dir /path/to/processed
```

**å¤šæœºå¤šå¡è®­ç»ƒï¼š**
```bash
# èŠ‚ç‚¹ 0 (Master)
torchrun --nnodes=2 --node_rank=0 --nproc_per_node=4 \
    --master_addr=10.0.0.1 --master_port=29500 \
    -m alphastomics.main train --config config.yaml

# èŠ‚ç‚¹ 1
torchrun --nnodes=2 --node_rank=1 --nproc_per_node=4 \
    --master_addr=10.0.0.1 --master_port=29500 \
    -m alphastomics.main train --config config.yaml
```

### 3. æµ‹è¯•æ¨¡å‹

```bash
python -m alphastomics.main test \
    --checkpoint outputs/checkpoints/best.ckpt \
    --data_dir /path/to/processed
```

### 4. é‡‡æ ·ç”Ÿæˆ

```bash
python -m alphastomics.main sample \
    --checkpoint outputs/checkpoints/best.ckpt \
    --input_data input.pt \
    --sample_mode expr_to_pos  # æˆ– pos_to_expr, joint
```

### 5. æå– Embedding

```bash
python -m alphastomics.main extract \
    --checkpoint outputs/checkpoints/best.ckpt \
    --data_dir /path/to/processed \
    --output embeddings.npz \
    --extract_mode transformer
```

---

## âš™ï¸ é…ç½®è¯´æ˜

ç¼–è¾‘ `config.yaml`ï¼š

```yaml
# è®­ç»ƒæ¨¡å¼
training_mode: "joint"  # "expr_to_pos", "pos_to_expr", "joint"

# æ‰©æ•£å‚æ•°
diffusion:
  diffusion_steps: 1000
  nu_expression: 1.0
  nu_position: 1.0

# æŸå¤±æƒé‡
loss:
  lambda_expression: 1.0
  lambda_position: 1.0
  use_distance_matrix: true  # æ—‹è½¬å¹³ç§»ä¸å˜æ€§

# æ•°æ®å‚æ•°
data:
  preprocessing:
    gene_list_file: null      # å›ºå®šåŸºå› åˆ—è¡¨
    position_key: "ccf"       # 3D åæ ‡ key
    position_is_3d: true      # å·²ç»æ˜¯ 3D åæ ‡
  loading:
    mode: "slice"             # slice(åˆ‡ç‰‡çº§) æˆ– cell(ç»†èƒçº§)

# åˆ†å¸ƒå¼è®­ç»ƒ
distributed:
  enabled: true
  strategy: "ddp"             # ddp, fsdp, deepspeed
  num_nodes: 2
  devices: 4
```

---

## ğŸ“Š ä¸¤ç§è®­ç»ƒæ¨¡å¼

| æ¨¡å¼ | è¯´æ˜ | batch_size å«ä¹‰ | é€‚ç”¨åœºæ™¯ |
|------|------|----------------|----------|
| `slice` | æ¯æ¬¡è¾“å…¥ä¸€æ•´å¼ åˆ‡ç‰‡ | æ¯æ‰¹åˆ‡ç‰‡æ•°é‡ | ä¿æŒåˆ‡ç‰‡å†…ç©ºé—´ç»“æ„ |
| `cell` | æ¯æ¬¡é‡‡æ ·å›ºå®šæ•°é‡ç»†èƒ | æ¯æ‰¹ç»†èƒæ•°é‡ | å¤§è§„æ¨¡æ•°æ®è®­ç»ƒ |

---

---

## ğŸ“ API ä½¿ç”¨ç¤ºä¾‹

### Python API

```python
from alphastomics import AlphaSTomicsModule, DiffusionSampler
from alphastomics.utils import create_dataloaders, SpatialDataPreprocessor

# 1. æ•°æ®é¢„å¤„ç†
preprocessor = SpatialDataPreprocessor(
    gene_list=['Gene1', 'Gene2', ...],  # å›ºå®šåŸºå› åˆ—è¡¨
    position_key='ccf',                  # 3D CCF åæ ‡
    position_is_3d=True,
)
metadata = preprocessor.preprocess_and_save(h5ad_files, output_dir)

# 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader, val_loader, test_loader, meta = create_dataloaders(
    data_dir=output_dir,
    mode='slice',  # æˆ– 'cell'
    batch_size=1,
)

# 3. è®­ç»ƒ
import pytorch_lightning as pl
model = AlphaSTomicsModule(cfg=config, num_genes=meta['n_genes'])
trainer = pl.Trainer(max_epochs=100, devices=4, strategy='ddp')
trainer.fit(model, train_loader, val_loader)

# 4. é‡‡æ ·
sampler = DiffusionSampler(model.model, model.noise_model)

# è¡¨è¾¾é‡ â†’ ä½ç½®ï¼ˆåŸå§‹ä»»åŠ¡ï¼‰
_, positions = sampler.sample(expr, pos, mask, mode="expr_to_pos")

# ä½ç½® â†’ è¡¨è¾¾é‡ï¼ˆæ–°ä»»åŠ¡ï¼‰
expression, _ = sampler.sample(expr, pos, mask, mode="pos_to_expr")

# è”åˆç”Ÿæˆ
expression, positions = sampler.sample(expr, pos, mask, mode="joint")
```

### æå– Embedding

```python
from alphastomics.utils import EmbeddingExtractor

extractor = EmbeddingExtractor(model, mode='transformer')
embeddings = extractor.extract(data_loader)

# embeddings åŒ…å«:
# - expression_embeddings: è¡¨è¾¾é‡ç‰¹å¾
# - position_embeddings: ä½ç½®ç‰¹å¾
# - cell_types: ç»†èƒç±»å‹æ ‡ç­¾
```

---

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

| ç±»åˆ« | æŒ‡æ ‡ | è¯´æ˜ |
|------|------|------|
| **è¡¨è¾¾é‡** | MSE, MAE, PCC, Spearman | é¢„æµ‹ç²¾åº¦ |
| **ä½ç½®** | Distance Matrix MSE, Procrustes | å‡ ä½•é‡å»ºç²¾åº¦ |
| **ç©ºé—´** | kNN Preservation, Moran's I | ç©ºé—´ç»“æ„ä¿æŒ |
| **èšç±»** | ARI, NMI, Accuracy | ç»†èƒç±»å‹èšç±»æ•ˆæœ |

---

## ğŸ“Š æ•°å­¦åŸç†

### å‰å‘æ‰©æ•£è¿‡ç¨‹

å¯¹äºæ—¶é—´æ­¥ $t \in [1, T]$ï¼š

$$z_t = \alpha_t \cdot x_0 + \sigma_t \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$

å…¶ä¸­ $\alpha_t$ å’Œ $\sigma_t$ åŸºäº cosine schedule è®¡ç®—ã€‚

### é€†å‘é‡‡æ ·è¿‡ç¨‹

ä» $z_T$ é€æ­¥å»å™ªåˆ° $z_0$ï¼š

$$z_s = c_1 \cdot z_t + c_2 \cdot \hat{x}_0 + \sigma_{t \to s} \cdot \epsilon$$

å…¶ä¸­ $\hat{x}_0$ æ˜¯æ¨¡å‹å¯¹åŸå§‹æ•°æ®çš„é¢„æµ‹ï¼ˆxâ‚€-predictionï¼‰ã€‚

### æŸå¤±å‡½æ•°

**è¡¨è¾¾é‡æŸå¤±** (MSE)ï¼š

$$\mathcal{L}_{expr} = \frac{1}{N \cdot G} \sum_{i,g} (x_{expr}^{(i,g)} - \hat{x}_{expr}^{(i,g)})^2$$

**ä½ç½®æŸå¤±** (è·ç¦»çŸ©é˜µ MSEï¼Œæ—‹è½¬å¹³ç§»ä¸å˜)ï¼š

$$\mathcal{L}_{pos} = \frac{1}{N^2} \sum_{i,j} (D_{true}^{(i,j)} - D_{pred}^{(i,j)})^2$$

å…¶ä¸­ $D^{(i,j)} = \|pos_i - pos_j\|_2$

---

## ğŸ”§ ç›¸å¯¹äº LUNA çš„æ”¹è¿›

| æ¨¡å— | LUNA | AlphaSTomics | æ”¹è¿›è¯´æ˜ |
|------|------|--------------|---------|
| **åæ ‡ç»´åº¦** | 2D | 3D CCF | æ”¯æŒé…å‡†åçš„ä¸‰ç»´ç©ºé—´æ•°æ® |
| **æ‰©æ•£æ¨¡æ€** | ä»…ä½ç½® | è¡¨è¾¾é‡ + ä½ç½® | åŒæ¨¡æ€è”åˆæ‰©æ•£ |
| **åŸºå› é€‰æ‹©** | é«˜å˜å¼‚åŸºå›  | å…¨åŸºå›  | æ”¯æŒå¤šå¹³å°æ•°æ®ç»Ÿä¸€ |
| **æ³¨æ„åŠ›** | Linear Attention | Linear Attention | O(nÂ²) â†’ O(n) |
| **é‡‡æ ·æ¨¡å¼** | å•å‘ | åŒå‘ + è”åˆ | æ›´çµæ´»çš„ç”Ÿæˆèƒ½åŠ› |
| **åˆ†å¸ƒå¼** | ä¸æ”¯æŒ | DDP/FSDP/DeepSpeed | å¤šæœºå¤šå¡è®­ç»ƒ |

---

## ğŸ“š å‚è€ƒ

- [LUNA: Tissue reassembly with generative AI](https://github.com/mlbio-epfl/LUNA)
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)

---

## ğŸ“„ è®¸å¯è¯

MIT License