# Gated Attention + MoE for AlphaSTomics

## ğŸ“š æ¦‚è¿°

æœ¬é¡¹ç›®åœ¨ AlphaSTomics ä¸­é›†æˆäº†ä¸¤ä¸ªå…³é”®æŠ€æœ¯æ”¹è¿›ï¼š

1. **Gated Attention**: æ›¿ä»£ Linear Attentionï¼Œæä¾›æ›´ç²¾ç¡®çš„ç©ºé—´å»ºæ¨¡èƒ½åŠ›
2. **Mixture of Experts (MoE)**: å¯é€‰çš„ FFN æ›¿ä»£æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡æ¨¡å‹å®¹é‡

---

## ğŸ¯ Gated Attention

### æ ¸å¿ƒä¼˜åŠ¿

| ç‰¹æ€§ | Linear Attention | Gated Attention |
|------|-----------------|-----------------|
| **å¤æ‚åº¦** | O(n) | O(nÂ²) |
| **ç²¾ç¡®æ€§** | è¿‘ä¼¼ | ç²¾ç¡® |
| **ç©ºé—´å»ºæ¨¡** | â–³ | âœ“âœ“ |
| **è®­ç»ƒç¨³å®šæ€§** | â–³ | âœ“âœ“ |
| **Attention Sink** | å­˜åœ¨ | è§£å†³ |
| **é€‚ç”¨åœºæ™¯** | è¶…é•¿åºåˆ— | Batch è®­ç»ƒ |

### ä½¿ç”¨æ–¹æ³•

åœ¨ `config.yaml` ä¸­å¯ç”¨ï¼š

```yaml
TransformerLayer_setting:
  use_gated_attention: true
  gate_type: "headwise"  # æˆ– "elementwise"
  use_qk_norm: true
```

### é—¨æ§ç±»å‹

- **headwise** (æ¨è): æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¸€ä¸ªæ ‡é‡ gate
  - å‚æ•°å°‘ï¼Œç¨³å®šæ€§å¥½
  - é€‚åˆå¤§å¤šæ•°åœºæ™¯
  
- **elementwise**: æ¯ä¸ªå…ƒç´ ä¸€ä¸ª gate
  - æ›´çµæ´»ï¼Œè¡¨è¾¾èƒ½åŠ›æ›´å¼º
  - å‚æ•°é‡è¾ƒå¤§

---

## ğŸ§  Mixture of Experts (MoE)

### æ ¸å¿ƒåŸç†

MoE ç”¨å¤šä¸ª"ä¸“å®¶"ç½‘ç»œæ›¿ä»£æ ‡å‡†çš„ FFNï¼š

```
æ ‡å‡† FFN:  Input â†’ Dense(d_ff) â†’ ReLU â†’ Dense(d_model) â†’ Output

MoE:       Input â†’ Router (é€‰æ‹© top-k ä¸“å®¶)
                 â†“
           Expert_1, Expert_2, ..., Expert_k
                 â†“
           åŠ æƒç»„åˆ â†’ Output
```

### æ ¸å¿ƒä¼˜åŠ¿

1. **å®¹é‡æå‡**: 8ä¸ªä¸“å®¶ â‰ˆ 8x FFN çš„å®¹é‡
2. **è®¡ç®—é«˜æ•ˆ**: æ¯ä¸ª token åªæ¿€æ´» top-k ä¸ªä¸“å®¶ï¼ˆé€šå¸¸ k=2ï¼‰
3. **ä¸“ä¸šåŒ–**: ä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒæ¨¡å¼
   - å¯èƒ½æŸäº›ä¸“å®¶ä¸“æ³¨è¡¨è¾¾é‡ï¼ŒæŸäº›ä¸“æ³¨ä½ç½®
   - æˆ–ä¸åŒä¸“å®¶è´Ÿè´£ä¸åŒç»†èƒç±»å‹/ç©ºé—´åŒºåŸŸ

### ä½¿ç”¨æ–¹æ³•

åœ¨ `config.yaml` ä¸­å¯ç”¨ï¼š

```yaml
TransformerLayer_setting:
  use_moe: true
  num_experts: 8       # ä¸“å®¶æ•°é‡
  moe_top_k: 2        # æ¯ä¸ª token æ¿€æ´»çš„ä¸“å®¶æ•°
  moe_load_balance_loss_weight: 0.01
```

### å‚æ•°å»ºè®®

| æ•°æ®é›†è§„æ¨¡ | num_experts | moe_top_k | è¯´æ˜ |
|-----------|-------------|-----------|------|
| å° (<10k æ ·æœ¬) | 4 | 1 | é¿å…è¿‡æ‹Ÿåˆ |
| ä¸­ (10k-100k) | 8 | 2 | å¹³è¡¡å®¹é‡å’Œæ•ˆç‡ |
| å¤§ (>100k) | 16 | 2 | å……åˆ†åˆ©ç”¨æ•°æ® |

### è´Ÿè½½å‡è¡¡

MoE åŒ…å«è¾…åŠ©æŸå¤±ï¼Œç¡®ä¿ä¸“å®¶è¢«å‡åŒ€ä½¿ç”¨ï¼š

```python
# è®­ç»ƒæ—¶è‡ªåŠ¨æ·»åŠ åˆ°æ€»æŸå¤±
total_loss = main_loss + moe_aux_loss
```

æƒé‡é€šè¿‡ `moe_load_balance_loss_weight` æ§åˆ¶ï¼ˆå»ºè®® 0.01-0.1ï¼‰ã€‚

---

## ğŸ”§ é…ç½®ç¤ºä¾‹

### 1. ä»… Gated Attentionï¼ˆæ¨èèµ·ç‚¹ï¼‰

```yaml
TransformerLayer_setting:
  use_gated_attention: true
  gate_type: "headwise"
  use_qk_norm: true
  use_moe: false
```

### 2. Gated Attention + MoEï¼ˆæœ€å¤§æ€§èƒ½ï¼‰

```yaml
TransformerLayer_setting:
  use_gated_attention: true
  gate_type: "headwise"
  use_qk_norm: true
  use_moe: true
  num_experts: 8
  moe_top_k: 2
  moe_load_balance_loss_weight: 0.01
```

### 3. ä»… MoEï¼ˆå¯¹æ¯”å®éªŒï¼‰

```yaml
TransformerLayer_setting:
  use_gated_attention: false
  use_moe: true
  num_experts: 8
  moe_top_k: 2
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### Gated Attention

| æŒ‡æ ‡ | é¢„æœŸæ”¹è¿› |
|------|---------|
| è¡¨è¾¾é‡é‡å»º MSE | -5% ~ -10% |
| ä½ç½® Distance MSE | -10% ~ -15% |
| è®­ç»ƒç¨³å®šæ€§ | æ”¯æŒæ›´å¤§å­¦ä¹ ç‡ |
| Masked Diffusion æ•ˆæœ | âœ“âœ“ |

### MoE

| æŒ‡æ ‡ | é¢„æœŸæ”¹è¿› |
|------|---------|
| æ¨¡å‹å®¹é‡ | +ï¼ˆnum_experts - 1ï¼‰Ã— FFN |
| è¡¨è¾¾é‡/ä½ç½®é‡å»º | -10% ~ -20% |
| è®­ç»ƒæ—¶é—´ | +20% ~ +30% |
| å‚æ•°é‡ | +ï¼ˆnum_experts - 1ï¼‰Ã— FFNå‚æ•° |

### Gated Attention + MoE

ç»„åˆä½¿ç”¨æ—¶ï¼Œæ•ˆæœå åŠ ï¼š
- è¡¨è¾¾é‡/ä½ç½®é‡å»º: -15% ~ -30%
- åœ¨å¤æ‚ä»»åŠ¡ä¸Šæå‡æ›´æ˜æ˜¾

---

## ğŸ§ª æ¶ˆèå®éªŒå»ºè®®

1. **Baseline**: æ ‡å‡†é…ç½®ï¼ˆLinear Attentionï¼Œæ—  MoEï¼‰
2. **+Gated**: æ·»åŠ  Gated Attention
3. **+MoE**: æ·»åŠ  MoEï¼ˆ4 experts, top-1ï¼‰
4. **+Both**: Gated Attention + MoE (8 experts, top-2)

æ¯ä¸ªé…ç½®è®­ç»ƒç›¸åŒçš„ epochsï¼Œè®°å½•ï¼š
- è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿
- è¡¨è¾¾é‡/ä½ç½®é‡å»ºè¯¯å·®
- è®­ç»ƒæ—¶é—´
- å‚æ•°é‡

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Gated Attention**: [Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://arxiv.org/abs/2505.06708)
   - NeurIPS 2025 Oral (Top 1.5%)
   - å·²è¢« Qwen3-Next é‡‡ç”¨

2. **Mixture of Experts**: 
   - [Switch Transformers](https://arxiv.org/abs/2101.03961)
   - [GShard](https://arxiv.org/abs/2006.16668)

---

## ğŸ› æ•…éšœæ’é™¤

### MoE è®­ç»ƒä¸ç¨³å®š

- é™ä½ `moe_load_balance_loss_weight`ï¼ˆå¦‚ 0.001ï¼‰
- å‡å°‘ä¸“å®¶æ•°é‡
- å¢åŠ  top-k

### æ˜¾å­˜ä¸è¶³

- å‡å°‘ `num_experts`
- ä½¿ç”¨ `gate_type: "headwise"` è€Œä¸æ˜¯ "elementwise"
- å‡å° batch_size

### æŸäº›ä¸“å®¶æœªè¢«ä½¿ç”¨

- å¢åŠ  `moe_load_balance_loss_weight`
- æ·»åŠ  noisy gatingï¼ˆå·²é»˜è®¤å¯ç”¨ï¼‰

---

## âœ… æµ‹è¯•

è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯å®ç°ï¼š

```bash
# æµ‹è¯• MoE æ¨¡å—
python test_moe.py

# æµ‹è¯•å®Œæ•´é›†æˆ
python test_gated_attention.py
```

---

## ğŸ’¡ ä½¿ç”¨å»ºè®®

1. **æ–°é¡¹ç›®**: ä» `use_gated_attention=true, use_moe=false` å¼€å§‹
2. **æ•°æ®å……è¶³**: å°è¯•æ·»åŠ  MoE (8 experts, top-2)
3. **è®¡ç®—å—é™**: ä»…ä½¿ç”¨ Gated Attention
4. **è¿½æ±‚æè‡´æ€§èƒ½**: ä¸¤è€…éƒ½å¯ç”¨

**Happy Training! ğŸš€**
