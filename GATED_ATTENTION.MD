# Gated Attention for AlphaSTomics

## ğŸ¯ æ¦‚è¿°

æˆ‘ä»¬ä¸º AlphaSTomics å¼•å…¥äº† **Gated Attention** æœºåˆ¶ï¼Œå®Œå…¨æ›¿ä»£åŸæœ‰çš„ Linear Attentionã€‚è¿™ä¸€æ”¹è¿›åŸºäº Qwen3 çš„ Gated Attention å®ç°ï¼Œä¸“é—¨é’ˆå¯¹ç©ºé—´è½¬å½•ç»„çš„åŒæ¨¡æ€æ‰©æ•£å»ºæ¨¡ä¼˜åŒ–ã€‚

### ä¸ºä»€ä¹ˆè¦æ›¿æ¢ Linear Attentionï¼Ÿ

| æ–¹é¢ | Linear Attention | Gated Attention | è¯´æ˜ |
|------|------------------|-----------------|------|
| **å¤æ‚åº¦** | O(n) | O(nÂ²) | Batch è®­ç»ƒä¸‹ n ä¸å¤§ï¼ŒO(nÂ²) å¯æ¥å— |
| **ç²¾ç¡®æ€§** | è¿‘ä¼¼ | ç²¾ç¡® | ç©ºé—´å…³ç³»å»ºæ¨¡éœ€è¦ç²¾ç¡®æ³¨æ„åŠ› |
| **åŠ¨æ€æ€§** | é™æ€ | åŠ¨æ€ï¼ˆquery-dependentï¼‰ | æ‰©æ•£ä¸åŒæ—¶é—´æ­¥éœ€è¦ä¸åŒå…³æ³¨ç‚¹ |
| **åŒæ¨¡æ€** | æ— æ˜¾å¼æ”¯æŒ | Gating æœºåˆ¶æ”¯æŒ | è¡¨è¾¾é‡å’Œä½ç½®éœ€è¦åŠ¨æ€å¹³è¡¡ |
| **Attention Sink** | å­˜åœ¨ | é¿å… | é˜²æ­¢æŸäº›ç»†èƒè¢«è¿‡åº¦å…³æ³¨ |
| **è®­ç»ƒç¨³å®šæ€§** | ä¸€èˆ¬ | ä¼˜ç§€ | æ”¯æŒæ›´å¤§å­¦ä¹ ç‡ |

---

## ğŸ”§ æ ¸å¿ƒå®ç°

### 1. GatedMultiHeadAttention

æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ› + é—¨æ§æœºåˆ¶ï¼š

```python
class GatedMultiHeadAttention(nn.Module):
    """
    Softmax Attention + Query-dependent Gating
    
    gate_type:
        - 'headwise': æ¯ä¸ªå¤´ä¸€ä¸ªæ ‡é‡ gate (æ¨è)
        - 'elementwise': æ¯ä¸ªå…ƒç´ ç‹¬ç«‹ gate
        - 'none': æ ‡å‡† Softmax Attention
    """
```

**æ ¸å¿ƒå…¬å¼**ï¼š

$$\text{Output} = \sigma(g(Q)) \odot \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

å…¶ä¸­ï¼š
- $g(Q)$: é—¨æ§å‡½æ•°ï¼ˆä¸ Q ä¸€èµ·ä»è¾“å…¥æŠ•å½±å¾—åˆ°ï¼‰
- $\sigma$: Sigmoid æ¿€æ´»
- $\odot$: é€å…ƒç´ ä¹˜æ³•ï¼ˆheadwise æ—¶å¯¹æ¯ä¸ªå¤´ï¼‰

### 2. GatedSelfAttentionModel

é€‚é… AlphaSTomics çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼š

```python
# ä¿æŒä¸åŸ SelfAttentionModel ç›¸åŒçš„æ¥å£
output_expression, output_positions, output_time = model(
    expression_features,  # (B, N, expr_dim)
    diffusion_time,       # (B, time_dim)
    position_features     # (B, N, 3)
)
```

**ç‰¹ç‚¹**ï¼š
- æ— ç¼æ›¿æ¢åŸ `SelfAttentionModel`
- ä¿ç•™ä½ç½®ç¼–ç ï¼ˆæ–¹å‘ + è·ç¦»ï¼‰
- æ”¯æŒæ—¶é—´æ­¥æ¡ä»¶

### 3. é›†æˆåˆ° TransformerLayer

```python
TransformerLayer(
    ...,
    use_gated_attention=True,  # å¯ç”¨ Gated Attention
    gate_type='headwise',       # é—¨æ§ç±»å‹
    use_qk_norm=True            # QK å½’ä¸€åŒ–
)
```

---

## ğŸ“ ä½¿ç”¨æ–¹æ³•

### 1. é…ç½®æ–‡ä»¶è®¾ç½®

ä¿®æ”¹ `config.yaml`ï¼š

```yaml
TransformerLayer_setting:
  num_layers: 6
  num_heads: 8
  ...
  
  # ğŸ†• Gated Attention é…ç½®
  use_gated_attention: true    # å¯ç”¨ï¼ˆæ¨èï¼‰
  gate_type: "headwise"        # headwise / elementwise / none
  use_qk_norm: true            # QK å½’ä¸€åŒ–ï¼ˆæå‡ç¨³å®šæ€§ï¼‰
```

### 2. è®­ç»ƒ

```bash
# ä½¿ç”¨ Gated Attention è®­ç»ƒ
python -m alphastomics.main train \
    --config config.yaml \
    --data_dir ./processed_data \
    --batch_size 1024

# å¯¹æ¯”å®éªŒï¼šå…³é—­ Gated Attention
# åœ¨ config.yaml ä¸­è®¾ç½® use_gated_attention: false
```

### 3. æµ‹è¯•å®ç°

```bash
# è¿è¡Œæµ‹è¯•è„šæœ¬
python test_gated_attention.py
```

æµ‹è¯•å°†éªŒè¯ï¼š
- âœ“ `GatedMultiHeadAttention` å„æ¨¡å¼å·¥ä½œæ­£å¸¸
- âœ“ `GatedSelfAttentionModel` è¾“å‡ºå½¢çŠ¶æ­£ç¡®
- âœ“ å®Œæ•´ `Model` å¯ä»¥æ­£å¸¸å‰å‘ä¼ æ’­
- âœ“ `AlphaSTomicsModule` å¯ä»¥æ­£å¸¸è®­ç»ƒ

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

åŸºäº Qwen3 è®ºæ–‡çš„ç»“è®ºï¼Œæˆ‘ä»¬é¢„æœŸåœ¨ AlphaSTomics ä¸Šçœ‹åˆ°ä»¥ä¸‹æ”¹è¿›ï¼š

### 1. è®­ç»ƒç¨³å®šæ€§

- **æ›´å¤§çš„å­¦ä¹ ç‡**ï¼šGating æä¾›æ›´å¥½çš„æ¢¯åº¦æµ
- **æ›´å¿«çš„æ”¶æ•›**ï¼šåŠ¨æ€è°ƒåˆ¶å‡å°‘å†—ä½™ä¿¡æ¯

### 2. é‡å»ºè´¨é‡

| æŒ‡æ ‡ | é¢„æœŸæ”¹è¿› | åŸå›  |
|------|---------|------|
| è¡¨è¾¾é‡ MSE | -5% ~ -10% | æ›´ç²¾ç¡®çš„ç»†èƒé—´å…³ç³»å»ºæ¨¡ |
| ä½ç½® Distance MSE | -10% ~ -15% | ç©ºé—´ç»“æ„çš„ç²¾ç¡®æ³¨æ„åŠ› |
| Masked é‡å»º | -15% ~ -20% | Gating + Masking ååŒæ•ˆæœ |

### 3. ç©ºé—´ç»“æ„ä¿æŒ

- **kNN Preservation**ï¼šæå‡ 5-10%
- **Moran's I**ï¼šç©ºé—´è‡ªç›¸å…³æ€§æ›´å¼º
- **æ—  Attention Sink**ï¼šæ‰€æœ‰ç»†èƒè¢«å¹³ç­‰å¯¹å¾…

### 4. åŒæ¨¡æ€å¯¹é½

- `joint` æ¨¡å¼ä¸‹è¡¨è¾¾é‡-ä½ç½®ä¸€è‡´æ€§æå‡
- ä¸åŒæ‰©æ•£æ—¶é—´æ­¥çš„åŠ¨æ€è°ƒæ•´èƒ½åŠ›

---

## ğŸ”¬ æ¶ˆèå®éªŒå»ºè®®

### å®éªŒè®¾è®¡

```yaml
# 1. Baseline: Linear Attention
use_gated_attention: false

# 2. Gated (headwise)
use_gated_attention: true
gate_type: "headwise"
use_qk_norm: true

# 3. Gated (elementwise)
use_gated_attention: true
gate_type: "elementwise"
use_qk_norm: true

# 4. Gated without QK norm
use_gated_attention: true
gate_type: "headwise"
use_qk_norm: false
```

### è¯„ä¼°æŒ‡æ ‡

**å¿…é¡»æŠ¥å‘Š**ï¼š
- è®­ç»ƒæŸå¤±æ›²çº¿
- éªŒè¯é›† MSEï¼ˆè¡¨è¾¾é‡ + ä½ç½®ï¼‰
- å‚æ•°é‡å’Œè®­ç»ƒæ—¶é—´

**æ¨èæŠ¥å‘Š**ï¼š
- kNN Preservation
- è·ç¦»çŸ©é˜µç›¸å…³æ€§
- ç»†èƒç±»å‹èšç±» ARI/NMI
- æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–ï¼ˆæ£€æŸ¥ attention sinkï¼‰

---

## ğŸ“ å‚æ•°é‡å¯¹æ¯”

ä»¥é»˜è®¤é…ç½®ä¸ºä¾‹ï¼š

```python
# å‡è®¾: d_model=256, num_heads=8, num_layers=6

# Linear Attention (baseline)
# - æ¯å±‚æ³¨æ„åŠ›: ~0.5M params

# Gated Attention (headwise)
# - æ¯å±‚æ³¨æ„åŠ›: ~0.52M params (+4%)
# - é¢å¤–: num_heads ä¸ª gate scores

# Gated Attention (elementwise)
# - æ¯å±‚æ³¨æ„åŠ›: ~0.75M params (+50%)
# - é¢å¤–: d_model ä¸ª gate scores
```

**æ¨è**ï¼š
- å°æ•°æ®é›†ï¼ˆ<10M ç»†èƒï¼‰ï¼šheadwise
- å¤§æ•°æ®é›†ï¼šå¯å°è¯• elementwise
- èµ„æºå—é™ï¼šheadwise å‡ ä¹æ— é¢å¤–å¼€é”€

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: OOM (Out of Memory)

**åŸå› **: O(nÂ²) æ³¨æ„åŠ›åœ¨å¤§ batch ä¸‹å†…å­˜æ¶ˆè€—é«˜

**è§£å†³**:
```yaml
training:
  batch_size: 512  # é™ä½ batch size
  accumulate_grad_batches: 2  # æ¢¯åº¦ç´¯ç§¯è¡¥å¿
```

### é—®é¢˜ 2: è®­ç»ƒä¸ç¨³å®š

**åŸå› **: é—¨æ§æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

**è§£å†³**:
```yaml
TransformerLayer_setting:
  use_qk_norm: true  # å¯ç”¨ QK å½’ä¸€åŒ–
  dropout: 0.2       # å¢åŠ  dropout

training:
  gradient_clip_val: 0.5  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
```

### é—®é¢˜ 3: æ€§èƒ½ä¸‹é™

**å¯èƒ½åŸå› **:
1. æ•°æ®é›†å¤ªå°ï¼ˆ<1000 ç»†èƒ/æ ·æœ¬ï¼‰
2. å­¦ä¹ ç‡ä¸åˆé€‚
3. Gate å…¨éƒ¨æ¥è¿‘ 0 æˆ– 1

**è°ƒè¯•**:
```python
# åœ¨ forward ä¸­æ·»åŠ æ—¥å¿—
gate_scores = torch.sigmoid(self.gate(...))
print(f"Gate mean: {gate_scores.mean()}, std: {gate_scores.std()}")
# å¥åº·èŒƒå›´: mean ~0.5, std ~0.2
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Qwen3 Gated Attention**  
   Qiu et al., "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free", NeurIPS 2025  
   https://arxiv.org/abs/2505.06708

2. **LUNA (åŸ AlphaSTomics åŸºç¡€)**  
   Tissue reassembly with generative AI  
   https://github.com/mlbio-epfl/LUNA

3. **Attention is All You Need**  
   Vaswani et al., 2017

---

## âœ… Checklist

è¿ç§»åˆ° Gated Attention å‰è¯·ç¡®è®¤ï¼š

- [ ] æ•°æ®å·²é¢„å¤„ç†ä¸º Parquet æ ¼å¼
- [ ] `config.yaml` ä¸­æ­£ç¡®è®¾ç½® `use_gated_attention: true`
- [ ] è¿è¡Œ `test_gated_attention.py` é€šè¿‡æ‰€æœ‰æµ‹è¯•
- [ ] å‡†å¤‡å¯¹æ¯”å®éªŒï¼ˆGated vs Linearï¼‰
- [ ] ç›‘æ§è®­ç»ƒæ—¶çš„ GPU å†…å­˜ä½¿ç”¨
- [ ] ä¿å­˜æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–åˆ†æ

---

## ğŸ¤ è´¡çŒ®

å¦‚æœä½ å‘ç°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿æäº¤ Issue æˆ– PRï¼

**ç‰¹åˆ«å…³æ³¨çš„ç‚¹**ï¼š
- ä¸åŒæ•°æ®é›†ä¸Šçš„æ•ˆæœå¯¹æ¯”
- å‚æ•°é«˜æ•ˆçš„ gating å˜ä½“
- æ³¨æ„åŠ›æƒé‡çš„ç”Ÿç‰©å­¦è§£é‡Š

---

**License**: MIT  
**ä½œè€…**: AlphaSTomics Team  
**æ›´æ–°æ—¥æœŸ**: 2025-12-08
